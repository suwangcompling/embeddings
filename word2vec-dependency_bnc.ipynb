{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import linecache\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import BNCCorpusReader\n",
    "from spacy.en import English\n",
    "from collections import defaultdict, Counter\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONE-TIME RUNS: PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parse bnc, write NOUN's as targets, VERB's as contexts in save_path\n",
    "\n",
    "# bnc_path = '/home/jacobsuwang/Documents/UTA2017/BAYES-DL/WORD2VEC/DATA/bnc/'\n",
    "# save_path = '/home/jacobsuwang/Documents/UTA2017/BAYES-DL/WORD2VEC/DATA/bnc_word_dep.txt'\n",
    "# bnc = BNCCorpusReader(root=bnc_path, fileids=r'\\w*.xml')\n",
    "\n",
    "# print '... loading BNC'\n",
    "# bnc_sents = [unicode(' '.join(sent).encode('ascii','ignore')) for sent in bnc.sents()]\n",
    "# parser = English()\n",
    "# count = 0\n",
    "# start = time.time()\n",
    "# with open(save_path,'a') as f:\n",
    "#     for sent in parser.pipe(bnc_sents, n_threads=32, batch_size=10000):\n",
    "#         count += 1\n",
    "#         for token in sent:\n",
    "#             if token.pos_=='NOUN':\n",
    "#                 f.write(token.lemma_+'\\t'+token.head.lemma_+'/'+token.dep_+'\\n')\n",
    "#             if token.head.pos_=='NOUN':\n",
    "#                 f.write(token.head.lemma_+'\\t'+token.lemma_+'/'+token.dep_+'-1\\n')\n",
    "#         if count%10000==0:\n",
    "#             end = time.time()\n",
    "#             print '   ...processed',count,'sentences (time elapsed: '+str(end-start)+' secs)'\n",
    "#             start = end\n",
    "#     print 'DONE! (in total',count,'sentences)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DONE! (in total 6019197 sentences)\n",
    "CPU times: user 1h 30min 29s, sys: 16.9 s, total: 1h 30min 46s\n",
    "Wall time: 38m\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # count NOUN's and VERB's\n",
    "\n",
    "# vr2count = Counter()\n",
    "# cpt2count = Counter()\n",
    "\n",
    "# count = 0\n",
    "# start = time.time()\n",
    "# with open(save_path,'r') as f:\n",
    "#     for line in f:\n",
    "#         count += 1\n",
    "#         line = line.split('\\t')\n",
    "#         if len(line)!=2: continue\n",
    "#         cpt,vr = line\n",
    "#         vr2count[vr] += 1\n",
    "#         cpt2count[cpt] += 1\n",
    "#         if count%100000==0:\n",
    "#             end = time.time()\n",
    "#             print '   ...counted',count,'entries (time elapsed: '+str(end-start)+' secs)'\n",
    "#             start = end\n",
    "#     print 'DONE! (in total',count,'entries)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DONE! (in total 54930729 entries)\n",
    "CPU times: user 46 s, sys: 604 ms, total: 46.6 s\n",
    "Wall time: 45.7 s\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # encode targets and contexts\n",
    "\n",
    "# cpt2i = {}\n",
    "# vr2i = {}\n",
    "# def get_id(item, dic):\n",
    "#     if item not in dic:\n",
    "#         dic[item] = len(dic)\n",
    "#     return dic[item]\n",
    "\n",
    "# count = 0\n",
    "# start = time.time()\n",
    "# cutoff = 10\n",
    "# bnc_word2vec = '/home/jacobsuwang/Documents/UTA2017/BAYES-DL/WORD2VEC/DATA/bnc_word2vec_dep.txt'\n",
    "# with open(save_path,'r') as source, open(bnc_word2vec,'a') as target:   \n",
    "#     for line in source:\n",
    "#         count += 1\n",
    "#         line = line.split('\\t')\n",
    "#         if len(line)!=2: continue\n",
    "#         cpt,vr = line\n",
    "#         if cpt2count[cpt]<cutoff or vr2count[vr]<cutoff: continue\n",
    "#         cpt_id, vr_id = get_id(cpt,cpt2i), get_id(vr,vr2i)\n",
    "#         target.write(str(cpt_id)+'\\t'+str(vr_id)+'\\n')\n",
    "#         if count%100000==0:\n",
    "#             end = time.time()\n",
    "#             print '   ...written',count,'entries (time elapsed: '+str(end-start)+' secs)'\n",
    "#             start = end\n",
    "#     print 'DONE! (in total',count,'entries)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DONE! (in total 54930729 entries)\n",
    "CPU times: user 1min 15s, sys: 588 ms, total: 1min 15s\n",
    "Wall time: 1min 15s\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save trackers\n",
    "\n",
    "# import cPickle\n",
    "# stats_path = '/home/jacobsuwang/Documents/UTA2017/BAYES-DL/WORD2VEC/DATA/bnc_word2vec_stats.p'\n",
    "# cPickle.dump((count,cpt2i,vr2i), open(stats_path,'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD2VEC (SKIP-GRAM, DEPENDENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "bnc_word2vec = '/home/jacobsuwang/Documents/UTA2017/BAYES-DL/WORD2VEC/DATA/bnc_word2vec_dep.txt'\n",
    "stats_path = '/home/jacobsuwang/Documents/UTA2017/BAYES-DL/WORD2VEC/DATA/bnc_word2vec_stats.p'\n",
    "count,cpt2i,vr2i = cPickle.load(open(stats_path,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linecache.clearcache()\n",
    "\n",
    "data_ids = range(count)\n",
    "random.shuffle(data_ids)\n",
    "\n",
    "class DataIterator:\n",
    "    \n",
    "    def __init__(self, data_ids):\n",
    "        self.data_ids = data_ids\n",
    "        self.epoch = 0\n",
    "        self.cursor = 0\n",
    "        self.size = len(data_ids)\n",
    "        \n",
    "    def get_batch(self, batch_ids):\n",
    "        batch, labels = [], []\n",
    "        for line_id in batch_ids:\n",
    "            line = linecache.getline(bnc_word2vec,line_id).split('\\t')\n",
    "            if len(line)!=2: continue\n",
    "            word_id,context_id = line\n",
    "            batch.append(int(word_id))\n",
    "            labels.append([int(context_id)])\n",
    "        return np.array(batch),np.array(labels)\n",
    "            \n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epoch += 1\n",
    "            self.cursor = 0\n",
    "            random.shuffle(self.data_ids)\n",
    "        batch_ids = self.data_ids[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return self.get_batch(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  110, 16648,  8663,  1989, 25158,   267,   125,  5378,   149]),\n",
       " array([[   285],\n",
       "        [   246],\n",
       "        [     4],\n",
       "        [   828],\n",
       "        [   158],\n",
       "        [ 26355],\n",
       "        [129769],\n",
       "        [   281],\n",
       "        [    58]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = DataIterator(data_ids)\n",
    "tt = t.next_batch(10)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15935377572505960904, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10498395341\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 16196948162637181691\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess: sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "target_size = len(cpt2i)\n",
    "context_size = len(vr2i)\n",
    "i2cpt = {i:cpt for cpt,i in cpt2i.iteritems()}\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "\n",
    "# valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "# valid_window = 1000  # Only pick dev samples in the head of the distribution.\n",
    "# valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "l = ['dog','cat','bird','animal',\n",
    "     'book','house','kitchen','room',\n",
    "     'pen','fork','government','news',\n",
    "     'price','money','computer','school']\n",
    "assert all(l_i in cpt2i for l_i in l)\n",
    "valid_examples = [cpt2i[cpt] for cpt in l]\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/gpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([target_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([context_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([context_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=context_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "num_epochs = 1\n",
    "num_steps = (int)((len(data_ids)/batch_size)*num_epochs)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    gen = DataIterator(data_ids)\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = gen.next_batch(batch_size)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 1000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = i2cpt[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = i2cpt[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LAST RESULTS\n",
    "\n",
    "Nearest to dog: horse, cat, rabbit, man, boy, pig, girl, animal,\n",
    "Nearest to cat: dog, rabbit, horse, pig, character, lad, monster, boy,\n",
    "Nearest to bird: animal, fish, specie, creature, rabbit, snake, insect, mammal,\n",
    "Nearest to animal: bird, fish, plant, creature, predator, dog, horse, specie,\n",
    "Nearest to book: poem, paper, volume, piece, man, kind, document, article,\n",
    "Nearest to house: villa, pub, palace, mansion, garage, man, castle, building,\n",
    "Nearest to kitchen: bathroom, bedroom, room, laboratory, garage, person, context, studio,\n",
    "Nearest to room: bedroom, space, kitchen, cabin, hall, place, strategy, time,\n",
    "Nearest to pen: paper, card, pencil, number, scheme, book, box, year,\n",
    "Nearest to fork: knife, gun, spoon, conception, plank, grin, hook, elbow,\n",
    "Nearest to government: administration, authority, regime, state, politician, parliament, king, time,\n",
    "Nearest to news: idea, story, outcome, evidence, player, message, piece, sound,\n",
    "Nearest to price: rate, value, cost, production, fee, number, wage, sale,\n",
    "Nearest to money: cash, stuff, sum, game, time, kind, fund, sort,\n",
    "Nearest to computer: machine, pc, system, microcomputer, database, printer, processor, question,\n",
    "Nearest to school: university, hospital, church, college, village, library, factory, form,\n",
    "('Average loss at step ', 7674200, ': ', 2.0442353295087816)\n",
    "('Average loss at step ', 7674400, ': ', 2.2573078954219818)\n",
    "('Average loss at step ', 7674600, ': ', 2.0706734832525253)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
